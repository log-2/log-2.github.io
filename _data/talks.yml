- type: Future Talks
  members:

      
- type: Past Talks
  members:
    - speaker: Konstanting Rusch (ETH Zurich)
      date: November 15, 2023
      title: "Physics-inspired Machine Learning"
      abstract: "Combining physics with machine learning is a rapidly growing field of research. Thereby, most work focuses on leveraging machine learning methods to solve problems in physics. Here, however, we focus on the converse, i.e., physics-inspired machine learning, which can be described as incorporating structure from physical systems into machine learning methods to obtain models with better inductive biases. More concretely, we propose several physics-inspired deep learning architectures for sequence modeling based on nonlinear coupled oscillators, Hamiltonian systems, and multi-scale dynamical systems. The proposed architectures tackle central problems in the field of recurrent sequence modeling, namely the vanishing and exploding gradients problem as well as the issue of insufficient expressive power. Moreover, we discuss physics-inspired learning on graphs, wherein the dynamics of the message-passing propagation are derived from physical systems. We further prove that these methods mitigate the over-smoothing issue, thereby enabling the construction of deep graph neural networks (GNNs). We extensively test all proposed methods on a variety of versatile synthetic and real-world datasets, ranging from image recognition, speech recognition, medical applications, and scientific computing for sequence models, to citation networks, computational chemistry applications, and networks of articles and websites for graph learning models."
      bio:
    - speaker: Bastian Rieck (Helmholtz Zentrum München)
      date: June 15, 2023
      title: "Curvature for Graph Learning"
      abstract: "Curvature bridges geometry and topology, using local information to derive global statements. While well-known in a differential topology context, it was recently extended to the domain of graphs. In fact, graphs give rise to various notions of curvature, which differ in expressive power and purpose. We will give a brief overview of curvature in graphs, define some relevant concepts, and show their utility for data science and machine learning applications. In particular, we shall discuss two applications: first, the use of curvature to *distinguish* between different models for synthesising new graphs from some unknown distribution; second, a novel *framework* for defining curvature for hypergraphs, whose structural properties require a more generic setting. We will also describe new applications that are specifically geared towards a treatment by curvature, thus underlining the utility of this concept for data science."
      bio: "https://bastian.rieck.me/"
    - speaker: Chaitanya K. Joshi (University of Cambridge)
      date: May 11, 2023
      title: "On the Expressive Power of Geometric Graph Neural Networks"
      abstract: "The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation."
      bio: "https://www.chaitjo.com/"
    - speaker: Csaba Toth (University of Oxford)
      date: May 25, 2023
      title: "Capturing Graphs with Hypo-Elliptic Diffusions"
      abstract: "Convolutional layers within graph neural networks operate by aggregating information about local neighbourhood structures; one common way to encode such substructures is through random walks. The distribution of these random walks evolves according to a diffusion equation defined using the graph Laplacian. We extend this approach by leveraging classic mathematical results about hypo-elliptic diffusions. This results in a novel tensor-valued graph operator, which we call the hypo-elliptic graph Laplacian. We provide theoretical guarantees and efficient low-rank approximation algorithms. In particular, this gives a structured approach to capture long-range dependencies on graphs that is robust to pooling. Besides the attractive theoretical properties, our experiments show that this method competes with graph transformers on datasets requiring long-range reasoning but scales only linearly in the number of edges as opposed to quadratically in nodes."
      bio: "https://www.maths.ox.ac.uk/people/csaba.toth"
    - speaker: Michael Hutchinson (University of Oxford)
      date: April 27, 2023
      title: "Riemannian Score-Based Generative Modelling"
      abstract: "Score-based generative models (SGMs) are a powerful class of generative models that exhibit remarkable empirical performance. Score-based generative modelling (SGM) consists of a “noising” stage, whereby a diffusion is used to gradually add Gaussian noise to data, and a generative model, which entails a “denoising” process defined by approximating the time-reversal of the diffusion. Existing SGMs assume that data is supported on a Euclidean space, i.e. a manifold with flat geometry. In many domains such as robotics, geoscience or protein modelling, data is often naturally described by distributions living on Riemannian manifolds and current SGM techniques are not appropriate. We introduce here Riemannian Score-based Generative Models (RSGMs), a class of generative models extending SGMs to Riemannian manifolds. We demonstrate our approach on a variety of manifolds, and in particular with earth and climate science spherical data."
      bio: "http://mjhutchinson.info/"
    - speaker: Francesco Di Giovanni (University of Cambridge)
      date: March 30, 2023
      title: "On over-squashing in Graph Neural Networks: from theoretical understanding to graph-rewiring solutions"
      abstract: "In this talk I will provide an overview on the over-squashing phenomenon for Message Passing Neural Networks (MPNN) and on different strategies that have been proposed to tackle this problem that fall under the class of graph-rewiring techniques. I will start by discussing existing works in the literature that have shaped the problem. I will then go through a recent paper where we provide a more rigorous theoretical understanding of over-squashing, connecting it to classical random-walk properties of the graph. This allows us to draw a unified framework to justify why different rewiring operations studied thus far, do indeed help mitigate over-squashing. I will then discuss a new work where we introduce notions of dynamic edge addition and delay mechanism, resulting in a novel way of treating the underlying graph which can enhance any existing MPNN and achieve strong performances on long-range tasks. Finally, I will try to focus on the bigger picture and on why I think these directions of research are important to understand if Graph Neural Networks are here to stay or not."
      bio: "https://francescodgv.github.io/"
    - speaker: Petar Veličković (DeepMind)
      date: March 7, 2023
      title: "Neural Algorithmic Reasoning"
      abstract: "Neural networks that are able to reliably execute algorithmic computation may hold transformative potential to both machine learning and theoretical computer science. On one hand, they could enable the kind of extrapolative generalisation scarcely seen with deep learning models. On another, they may allow for running classical algorithms on inputs previously considered inaccessible to them. Both of these promises are shepherded by the neural algorithmic reasoning blueprint, which has been recently proposed in a position paper I co-authored with Charles Blundell. On paper, this is a remarkably elegant pipeline for reasoning on natural inputs which carefully leverages the tried-and-tested power of deep neural networks as feature extractors. In practice, how far did we actually take it? In this tutorial, we aim to provide the foundations needed to answer three key questions of neural algorithmic reasoning: how to develop neural networks that execute algorithmic computation, how to deploy such neural networks in real-world problems, and how to deepen their theoretical links to classical algorithms."
      bio: "https://petar-v.com/"
    - speaker: Fabrizio Frasca (Imperial College London)
      date: March 6, 2023
      title: "Pack your subgraphs - A journey into subgraphs for powerful Graph Neural Networks"
      abstract: "Whilst considered the leading architectures for Deep Learning on graph-structured data, Message Passing Neural Networks (MPNNs) are intrinsically bounded in their expressive power by the WL heuristic for graph isomorphism testing - a limitation that has recently ignited prolific research aiming at breaking this fundamental bottleneck. Yet, the most prominent approaches either suffer from high computational complexity and weak inductive biases or require some form of domain knowledge for their effective application. In this talk - which I structure in two parts - I will argue that these aforementioned limitations can be mitigated by resorting to a principled modelling of subgraphs. We will first observe that graphs not distinguishable by MPNNs often contain distinguishable subgraphs. In the first part of the talk, we will build upon this intuition to design a novel framework dubbed Equivariant Subgraph Aggregation Networks (ESAN), prescribing to represent a graph as a bag of subgraphs returned by a pre-specified policy and to process it with a suitable equivariant architecture. We show that this approach effectively increases the expressive power of both MPNNs and more expressive architectures, and study the impact of its design choices. Interestingly, we notice a surge in concurrent approaches which - sometimes unwittingly - make use of subgraphs for powerful graph representations. In the second part of the talk we will focus on the most prominent form of these methods: those where subgraphs are directly “generated” by nodes in the original graph (node-based methods). A novel symmetry analysis allows to unify and better characterise this class of approaches: we prove an upper-bound on their expressive power and conceive a framework serving as a design space for equivariant node-based subgraph architectures. Finally, we introduce a novel instantiation of this framework: a new method dubbed SUN, which captures previous architectures while providing better empirical performance on multiple benchmarks."
      bio: "https://noired.github.io/"   


#- type: Past Talks
#  members:
#   - speaker: Petar Veličković (DeepMind)
#      date: March 7, 2023
#      title: "Neural Algorithmic Reasoning"
#      abstract: "Neural networks that are able to reliably execute algorithmic computation may hold transformative potential to both machine learning and theoretical computer science. On one hand, they could enable the kind of extrapolative generalisation scarcely seen with deep learning models. On another, they may allow for running classical algorithms on inputs previously considered inaccessible to them. Both of these promises are shepherded by the neural algorithmic reasoning blueprint, which has been recently proposed in a position paper I co-authored with Charles Blundell. On paper, this is a remarkably elegant pipeline for reasoning on natural inputs which carefully leverages the tried-and-tested power of deep neural networks as feature extractors. In practice, how far did we actually take it? In this tutorial, we aim to provide the foundations needed to answer three key questions of neural algorithmic reasoning: how to develop neural networks that execute algorithmic computation, how to deploy such neural networks in real-world problems, and how to deepen their theoretical links to classical algorithms."
#      bio: "https://petar-v.com/"
#      link: "https://arxiv.org/abs/2105.02761"
#    - speaker: Fabrizio Frasca (Imperial College London)
#      date: March 6, 2023
#      title: "Pack your subgraphs - A journey into subgraphs for powerful Graph Neural Networks"
#      abstract: "Whilst considered the leading architectures for Deep Learning on graph-structured data, Message Passing Neural Networks (MPNNs) are intrinsically bounded in their expressive power by the WL heuristic for graph isomorphism testing - a limitation that has recently ignited prolific research aiming at breaking this fundamental bottleneck. Yet, the most prominent approaches either suffer from high computational complexity and weak inductive biases or require some form of domain knowledge for their effective application. In this talk - which I structure in two parts - I will argue that these aforementioned limitations can be mitigated by resorting to a principled modelling of subgraphs. We will first observe that graphs not distinguishable by MPNNs often contain distinguishable subgraphs. In the first part of the talk, we will build upon this intuition to design a novel framework dubbed Equivariant Subgraph Aggregation Networks (ESAN), prescribing to represent a graph as a bag of subgraphs returned by a pre-specified policy and to process it with a suitable equivariant architecture. We show that this approach effectively increases the expressive power of both MPNNs and more expressive architectures, and study the impact of its design choices. Interestingly, we notice a surge in concurrent approaches which - sometimes unwittingly - make use of subgraphs for powerful graph representations. In the second part of the talk we will focus on the most prominent form of these methods: those where subgraphs are directly “generated” by nodes in the original graph (node-based methods). A novel symmetry analysis allows to unify and better characterise this class of approaches: we prove an upper-bound on their expressive power and conceive a framework serving as a design space for equivariant node-based subgraph architectures. Finally, we introduce a novel instantiation of this framework: a new method dubbed SUN, which captures previous architectures while providing better empirical performance on multiple benchmarks."
#      bio: "https://noired.github.io/"
#      link: https://arxiv.org/abs/2110.02910
#      link2: https://arxiv.org/abs/2206.11140

    

 
